{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor.eye(3)\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor <LB METAL (3, 1) contig:False ShapeTracker(views=(View(shape=(3, 1), strides=(0, 0), offset=0, mask=None, contiguous=False),))> on METAL with grad None>\n",
      "[[1]\n",
      " [1]\n",
      " [1]] full\n",
      "<Tensor <LB METAL (3, 4) contig:False ShapeTracker(views=(View(shape=(3, 4), strides=(0, 0), offset=0, mask=((0, 3), (0, 1)), contiguous=False),))> on METAL with grad None>\n",
      "[[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]] pad\n",
      "<Tensor <LB METAL (12,) contig:False ShapeTracker(views=(View(shape=(3, 4), strides=(0, 0), offset=0, mask=((0, 3), (0, 1)), contiguous=False), View(shape=(12,), strides=(1,), offset=0, mask=None, contiguous=True)))> on METAL with grad None>\n",
      "[1 0 0 0 1 0 0 0 1 0 0 0] reshape\n",
      "<Tensor <LB METAL (9,) contig:False ShapeTracker(views=(View(shape=(3, 4), strides=(0, 0), offset=0, mask=((0, 3), (0, 1)), contiguous=False), View(shape=(9,), strides=(1,), offset=0, mask=None, contiguous=True)))> on METAL with grad None>\n",
      "[1 0 0 0 1 0 0 0 1] shrink\n",
      "<Tensor <LB METAL (3, 3) contig:False ShapeTracker(views=(View(shape=(3, 4), strides=(0, 0), offset=0, mask=((0, 3), (0, 1)), contiguous=False), View(shape=(3, 3), strides=(3, 1), offset=0, mask=None, contiguous=True)))> on METAL with grad None>\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]] reshape\n"
     ]
    }
   ],
   "source": [
    "dim = 3\n",
    "b = Tensor.full((dim, 1), 1)\n",
    "# args = ((0,0),(0,dim))\n",
    "# print(tuple([(p[0], s+p[1]) for s,p in zip(b.shape, args)]))\n",
    "# print(tuple(x if x is not None else (1,1) for x in args))\n",
    "print(b)\n",
    "print(f\"{b.numpy()} full\")\n",
    "# arg = ((top, bottom),(left, right))\n",
    "b = b.pad(((0,0),(0,dim)))\n",
    "print(b)\n",
    "print(f\"{b.numpy()} pad\")\n",
    "b = b.reshape(dim*(dim+1))\n",
    "print(b)\n",
    "print(f\"{b.numpy()} reshape\")\n",
    "b = b.shrink(((0, dim*dim),))\n",
    "print(b)\n",
    "print(f\"{b.numpy()} shrink\")\n",
    "b = b.reshape(dim, dim)\n",
    "print(b)\n",
    "print(f\"{b.numpy()} reshape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor <LB METAL (5,) contig:True (<LoadOps.COPY: 3>, None)> on METAL with grad None>\n",
      "<Tensor <LB METAL (5,) contig:False ShapeTracker(views=(View(shape=(5,), strides=(1,), offset=0, mask=((0, 2),), contiguous=False),))> on METAL with grad None>\n",
      "[1 1 1 1 1] [1 1 0 0 0]\n",
      "<Tensor <LB METAL (5,) contig:True (<BinaryOps.MUL: 3>, None)> on METAL with grad None>\n",
      "[1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,1,1,1,1])\n",
    "b = Tensor([1,1]).pad(((0,3),))\n",
    "print(a)\n",
    "print(b)\n",
    "print(a.numpy(), b.numpy())\n",
    "c = a*b\n",
    "print(c)\n",
    "print(c.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 2.5 1.5811388\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1, 2, 3, 4, 5])\n",
    "print(a.mean().numpy(), a.var().numpy(), a.std().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "m = a.sum()/a.numel()\n",
    "assert a.mean().numpy() == m.numpy()\n",
    "\n",
    "# var\n",
    "correction = 1\n",
    "ss = (a- m).square().sum()\n",
    "v = ss.div(max(0, a.numel() - correction))\n",
    "assert a.var().numpy() == v.numpy()\n",
    "\n",
    "# std\n",
    "std = v.sqrt()\n",
    "assert a.std().numpy() == std.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,) 5\n",
      "[False  True  True  True False] [0 2 3 4 0]\n"
     ]
    }
   ],
   "source": [
    "print(a.shape, *a.shape)\n",
    "u = Tensor.rand(*a.shape) < 0.5\n",
    "b = a * u\n",
    "print(u.numpy(), b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tinygrad.helpers import make_pair\n",
    "stride = 1\n",
    "b = Tensor.ones((1, 3, 16, 16))\n",
    "print(len(b.shape))\n",
    "make_pair(stride, len(b.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5] (5,) 1\n",
      "[5 0 0 0 0 0 0]\n",
      "c\n",
      "[0 1 2 3 4 5 0]\n",
      "[0 1 2 3 4 5 0 0]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1, 2, 3, 4, 5])\n",
    "print(a.numpy(), a.shape, len(a.shape))\n",
    "print(a.slice([(4,11)]).numpy())\n",
    "b = Tensor.ones((2, 5))\n",
    "b = b.pad([(2,2), (1, 1)])\n",
    "print(\"c\")\n",
    "c = a.pad([(1, 1)])\n",
    "print(c.numpy())\n",
    "# print(c.shrink([(1, 6)]).numpy())\n",
    "print(c.pad([(0, 1)]).numpy())\n",
    "\n",
    "# print(b.numpy())\n",
    "# print(b.shrink([(2, 4), (1, 6)]).numpy())\n",
    "# print(b.slice([(2, 4), (1, 15)]).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2, 110, 28) (3, 3) (3, 2) 1\n",
      "s  (3, 2)\n",
      "noop  [None, None]\n",
      "i  (110, 28)\n",
      "o  [36, 13]\n",
      "repeat (32, 2, 440, 112) [4, 4]\n",
      "handle dilation\n",
      "slice (32, 2, 333, 87)\n",
      "reshape (32, 2, 3, 111, 3, 29)\n",
      "handle stride\n",
      "slice (32, 2, 3, 108, 3, 26)\n",
      "reshape (32, 2, 3, 36, 3, 3, 13, 2)\n",
      "slice (32, 2, 3, 36, 1, 3, 13, 1)\n",
      "reshape (32, 2, 3, 36, 3, 13)\n",
      "permute\n",
      "(32, 2, 36, 13, 3, 3) (bs, groups*cin, oy, ox, H, W)\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Union\n",
    "from tinygrad.helpers import argfix, make_pair, flatten, prod, all_int, round_up, merge_dicts, fully_flatten, flat_mv, argsort\n",
    "import math\n",
    "sint = int\n",
    "\n",
    "def _pool(self, k_:Tuple[sint, ...], stride:Union[Tuple[int, ...], int]=1, dilation:Union[Tuple[int, ...], int]=1) -> Tensor:\n",
    "  print(self.shape, k, stride, dilation)\n",
    "  assert len(self.shape) >= len(k_), f\"can't pool {self.shape} with {k_}\"\n",
    "  assert all_int(self.shape) and all_int(k_), f\"does not support symbolic {self.shape=}, {k_=}\"\n",
    "  s_, d_ = make_pair(stride, len(k_)), make_pair(dilation, len(k_))\n",
    "  print(\"s \", s_)\n",
    "  assert len(k_) == len(s_) == len(d_), f\"stride/dilation mismatch kernel:{k_} stride:{s_} dilation:{d_}\"\n",
    "  noop_, i_ = [None] * len(self.shape[:-len(k_)]), self.shape[-len(k_):]\n",
    "  print(\"noop \", noop_)\n",
    "  print(\"i \", i_)\n",
    "  if any(k > s for k,s in zip(k_, s_)) or any(d != 1 for d in d_):\n",
    "    o_ = [(i - d * (k-1) - 1)//s + 1 for i,d,k,s in zip(i_, d_, k_, s_)]\n",
    "    print(\"o \", o_)\n",
    "    # repeats such that we don't need padding\n",
    "    xup = self.repeat([1]*len(noop_) + [math.ceil(k*(i+d) / i) for k,i,d in zip(k_, i_, d_)])\n",
    "    print(\"repeat\", xup.shape, [math.ceil(k*(i+d) / i) for k,i,d in zip(k_, i_, d_)])\n",
    "    # slice by dilation\n",
    "    print(\"handle dilation\")\n",
    "    xup = xup.shrink(noop_ + [(0,k*(i+d)) for k,i,d in zip(k_, i_, d_)])\n",
    "    print(\"slice\", xup.shape)\n",
    "    xup = xup.reshape(noop_ + flatten((k,i+d) for k,i,d in zip(k_, i_, d_)))\n",
    "    print(\"reshape\", xup.shape)\n",
    "    # handle stride\n",
    "    print(\"handle stride\")\n",
    "    xup = xup.shrink(noop_ + flatten(((0,k), (0,o*s)) for k,o,s in zip(k_, o_, s_)))\n",
    "    print(\"slice\", xup.shape)\n",
    "    xup = xup.reshape(noop_ + flatten((k,o,s) for k,o,s in zip(k_, o_, s_)))\n",
    "    print(\"reshape\", xup.shape)\n",
    "    xup = xup.shrink(noop_ + flatten(((0,k), (0,o), (0,1)) for k,o in zip(k_, o_)))\n",
    "    print(\"slice\", xup.shape)\n",
    "    xup = xup.reshape(noop_ + flatten((k,o) for k,o in zip(k_, o_)))\n",
    "    print(\"reshape\", xup.shape)\n",
    "    # permute to move reduce to the end\n",
    "    print(\"permute\")\n",
    "    return xup.permute(*range(len(noop_)), *[len(noop_)+i*2+1 for i in range(len(i_))], *[len(noop_)+i*2 for i in range(len(i_))])\n",
    "  # TODO: once the shapetracker can optimize well, remove this alternative implementation. or not if the CPU implementation doesn't use ShapeTracker\n",
    "  o_ = [(i+(s-k))//s for i,s,k in zip(i_, s_, k_)]\n",
    "  print(\"outer o\", o_)\n",
    "  # TODO: remove slice and use pad and shrink\n",
    "  # xup = self.slice(noop_ + [(0,o*s) for o,s in zip(o_, s_)])\n",
    "  print(self.shape, [(0, max(0,o*s-i)) for i,o,s in zip(i_, o_, s_)])\n",
    "  xup = self.pad(noop_ + [(0, max(0,o*s-i)) for i,o,s in zip(i_, o_, s_)])\n",
    "  print(\"pad\", xup.shape)\n",
    "  xup = xup.shrink(noop_ + [(0,o*s) for o,s in zip(o_, s_)])  \n",
    "  # xup = xup.shrink(noop_ + arg)\n",
    "  print(\"outer slice\", xup.shape)\n",
    "  # idxs = noop_ + [(0,o*s) for o,s in zip(o_, s_)]\n",
    "  # print(idxs)\n",
    "  # xup = self.pad(tuple((max(0,-a+b[0]), max(0,b[1]-a)) for a,b in zip(i_,idxs))).shrink(idxs)\n",
    "  # print(noop_, xup.numpy(), [(0,o*s) for o,s in zip(o_, s_)])\n",
    "  xup = xup.reshape(noop_ + flatten(((o,s) for o,s in zip(o_, s_))))\n",
    "  print(\"outer reshape\", xup.shape)\n",
    "  xup = xup.shrink(noop_ + flatten(((0,o), (0,k)) for o,k in zip(o_, k_)))\n",
    "  print(\"outer slice\", xup.shape)\n",
    "  return xup.permute(*range(len(noop_)), *[len(noop_)+i*2 for i in range(len(i_))], *[len(noop_)+i*2+1 for i in range(len(i_))])\n",
    "\n",
    "a = Tensor.ones((32,2,110,28))\n",
    "k = (3, 3)\n",
    "stride = (3,2)\n",
    "dilation = 1\n",
    "c = _pool(a, k, stride, dilation)\n",
    "print(c.shape, \"(bs, groups*cin, oy, ox, H, W)\")\n",
    "# print(c.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pool(self, k_:Tuple[sint, ...], stride:Union[Tuple[int, ...], int]=1, dilation:Union[Tuple[int, ...], int]=1) -> Tensor:\n",
    "    assert len(self.shape) >= len(k_), f\"can't pool {self.shape} with {k_}\"\n",
    "    assert all_int(self.shape) and all_int(k_), f\"does not support symbolic {self.shape=}, {k_=}\"\n",
    "    s_, d_ = make_pair(stride, len(k_)), make_pair(dilation, len(k_))\n",
    "    assert len(k_) == len(s_) == len(d_), f\"stride/dilation mismatch kernel:{k_} stride:{s_} dilation:{d_}\"\n",
    "    noop_, i_ = [None] * len(self.shape[:-len(k_)]), self.shape[-len(k_):]\n",
    "    if any(k > s for k,s in zip(k_, s_)) or any(d != 1 for d in d_):\n",
    "      o_ = [(i - d * (k-1) - 1)//s + 1 for i,d,k,s in zip(i_, d_, k_, s_)]\n",
    "      # repeats such that we don't need padding\n",
    "      xup = self.repeat([1]*len(noop_) + [math.ceil(k*(i+d) / i) for k,i,d in zip(k_, i_, d_)])\n",
    "      # slice by dilation\n",
    "      xup = xup.shrink(noop_ + [(0,k*(i+d)) for k,i,d in zip(k_, i_, d_)]).reshape(noop_ + flatten((k,i+d) for k,i,d in zip(k_, i_, d_)))\n",
    "      # handle stride\n",
    "      xup = xup.shrink(noop_ + flatten(((0,k), (0,o*s)) for k,o,s in zip(k_, o_, s_))).reshape(noop_ + flatten((k,o,s) for k,o,s in zip(k_, o_, s_)))\n",
    "      xup = xup.shrink(noop_ + flatten(((0,k), (0,o), (0,1)) for k,o in zip(k_, o_))).reshape(noop_ + flatten((k,o) for k,o in zip(k_, o_)))\n",
    "      # permute to move reduce to the end\n",
    "      return xup.permute(*range(len(noop_)), *[len(noop_)+i*2+1 for i in range(len(i_))], *[len(noop_)+i*2 for i in range(len(i_))])\n",
    "    # TODO: once the shapetracker can optimize well, remove this alternative implementation. or not if the CPU implementation doesn't use ShapeTracker\n",
    "    o_ = [(i+(s-k))//s for i,s,k in zip(i_, s_, k_)]\n",
    "    xup = self.pad(noop_ + [(0, max(0,o*s-i)) for i,o,s in zip(i_, o_, s_)]).shrink(noop_ + [(0,o*s) for o,s in zip(o_, s_)])\n",
    "    xup = xup.reshape(noop_ + flatten(((o,s) for o,s in zip(o_, s_))))\n",
    "    xup = xup.shrink(noop_ + flatten(((0,o), (0,k)) for o,k in zip(o_, k_)))\n",
    "    return xup.permute(*range(len(noop_)), *[len(noop_)+i*2 for i in range(len(i_))], *[len(noop_)+i*2+1 for i in range(len(i_))])\n",
    "\n",
    "\n",
    "def test_maxpool2d_bigger_stride_dilation(self):\n",
    "    for stride, dilation in zip([(2,3), (3,2), 2, 3, 4], [(3,2), (2,3), 2, 3, 6]):\n",
    "      with self.subTest(stride=stride):\n",
    "        helper_test_op([(32,2,110,28)],\n",
    "          lambda x: torch.nn.functional.max_pool2d(x, kernel_size=(2,2), stride=stride, dilation=dilation),\n",
    "          lambda x: Tensor.max_pool2d(x, kernel_size=(2,2), stride=stride, dilation=dilation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pair(k, a.shape)\n",
    "assert a.avg_pool2d((3,3),1,0).shape == c.mean(axis=(-2,-1)).shape\n",
    "assert a.max_pool2d((3,3),1,0).shape == c.max(axis=(-2,-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 18, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Tensor <LB METAL (1, 3, 3, 6, 3, 6) contig:False ShapeTracker(views=(View(shape=(1, 3, 3, 6, 3, 6), strides=(0, 0, 0, 0, 0, 0), offset=0, mask=None, contiguous=False),))> on METAL with grad None>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor.ones((1, 3, 6, 6))\n",
    "a = a.repeat((1, 1, 3, 3))\n",
    "print(a.shape)\n",
    "a.reshape(1, 3, 3, 6, 3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (0, 0), (1, 1), (0, 0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ((1,1), (1,1))\n",
    "b = ((0,0), (0,0))\n",
    "flatten((x,y) for x,y in zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "i,d,k,s = 6,0,3,2\n",
    "res = (i - d * (k-1) - 1)//s + 1\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [2, 2, 2, 2]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor([[1], [2,3 ]])\n",
    "a.expand((a.shape[0], 4)).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
